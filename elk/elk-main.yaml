apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: elasticsearch
  namespace: monitoring-elk
  labels:
    app: elasticsearch
    stack: logging
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: elasticsearch
        stack: logging
    spec:
      serviceAccountName: elasticsearch
      initContainers:
      - name: set-vm-max-map-count
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ['sysctl', '-w', 'vm.max_map_count=262144']
        securityContext:
          privileged: true
      - name: volume-mount-hack
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:100 /usr/share/elasticsearch/data"]
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.0.0
        imagePullPolicy: IfNotPresent
        env:
        - name: ES_JAVA_OPTS
          value: -Xms1024m -Xmx1024m
          # ES_MEM_LIMIT=2g
          # ES_JVM_HEAP=1024m
        ports:
        - containerPort: 9200
        resources:
          limits:
            memory: "2147483648"
        volumeMounts:
        - name: config
          mountPath: /usr/share/elasticsearch/elasticsearch.yml
          subPath: elasticsearch.yml
        - name: data
          mountPath: /usr/share/elasticsearch/data
      # Allow non-root user to access PersistentVolume
      securityContext:
        fsGroup: 1000
      restartPolicy: Always
      volumes:
      - name: config
        configMap:
          name: elasticsearch
      - name: data
        persistentVolumeClaim:
          claimName: elasticsearch
      # - name: data
      #   hostPath:
      #     path: /srv/elasticsearch-data
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: monitoring-elk
  labels:
    app: elasticsearch
    stack: logging
spec:
  # needed on minikube
  type: NodePort
  ports:
  - name: "api"
    port: 9200
    targetPort: 9200
  selector:
    app: elasticsearch
    stack: logging
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: monitoring-elk
  labels:
    app: fluentd
spec:
  template:
    metadata:
      name: fluentd
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: gcr.io/google-containers/fluentd-elasticsearch:v2.3.1
        imagePullPolicy: IfNotPresent
        env:
        - name: OUTPUT_HOST
          value: 'elasticsearch.monitoring-elk.svc'
        - name: OUTPUT_PORT
          value: '9200'
        - name: OUTPUT_BUFFER_CHUNK_LIMIT
          value: '2M'
        - name: OUTPUT_BUFFER_QUEUE_LIMIT
          value: '8'
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: libsystemddir
          mountPath: /host/lib
          readOnly: true
        - name: config-volume-fluentd
          mountPath: /etc/fluent/config.d
        # Liveness probe is aimed to help in situarions where fluentd
        # silently hangs for no apparent reasons until manual restart.
        # The idea of this probe is that if fluentd is not queueing or
        # flushing chunks for 5 minutes, something is not right. If
        # you want to change the fluentd configuration, reducing amount of
        # logs fluentd collects, consider changing the threshold or turning
        # liveness probe off completely.
        livenessProbe:
          initialDelaySeconds: 600
          periodSeconds: 60
          exec:
            command:
            - '/bin/sh'
            - '-c'
            - >
              LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300};
              STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};
              if [ ! -e /var/log/fluentd-buffers ];
              then
                exit 1;
              fi;
              touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck;
              if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]];
              then
                rm -rf /var/log/fluentd-buffers;
                exit 1;
              fi;
              touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness;
              if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]];
              then
                exit 1;
              fi;
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      # It is needed to copy systemd library to decompress journals
      - name: libsystemddir
        hostPath:
          path: /usr/lib64
      - name: config-volume-fluentd
        configMap:
          name: fluentd
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kibana
  namespace: monitoring-elk
  labels:
    app: kibana
    stack: logging
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: kibana
        stack: logging
    spec:
      # FIXME
      # healthcheck + resources
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana-oss:6.0.0
        imagePullPolicy: IfNotPresent
        env:
        - name: ELASTICSEARCH_PASSWORD
          value: changeme
        ports:
        - containerPort: 5601
        resources: {}
        volumeMounts:
        - name: config
          mountPath: /usr/share/kibana/kibana.yml
          subPath: kibana.yml
      restartPolicy: Always
      volumes:
      - name: config
        configMap:
          name: kibana
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: monitoring-elk
  labels:
    app: kibana
    stack: logging
spec:
  type: LoadBalancer
  ports:
  - name: "ui"
    port: 5601
  selector:
    app: kibana
    stack: logging
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  namespace: monitoring-elk
  name: oauth2-proxy
  labels:
    app: oauth2-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: oauth2-proxy
  template:
    metadata:
      labels:
        app: oauth2-proxy
    spec:
      containers:
      - name: oauth2-proxy
        image: giantswarm/oauth2_proxy:master
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 4180
          protocol: TCP
        volumeMounts:
        - name: config
          mountPath: /etc/oauth2_proxy
      volumes:
      - name: config
        configMap:
          name: oauth2-proxy
---
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring-elk
  name: oauth2-proxy
  labels:
    app: oauth2-proxy
spec:
  ports:
  - name: http
    port: 4180
    protocol: TCP
    targetPort: 4180
  selector:
    app: oauth2-proxy